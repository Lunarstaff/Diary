- 最开始的想法

目标网址：`http://www.nenmp.com/30`页面第一个图集的所有图片：  
第一个图集：`/html/body/div[1]/div[1]/a[1]`  
第二个图集：`/html/body/div[1]/div[1]/a[2]`  
.  
.  
.  
第十个图集：`/html/body/div[1]/div[1]/a[10]`  

- #### 一、[项目](./image_pro/image_pro_v1.py)的大致过程 
    - 首先建立图集类`Album`
        - 类属性：
        - 名称（ab_name）
        - 链接（ab_link）
        - 保存路径（ab_path）
        - 图集中图片文件名前缀（image_name_pre）
    ```python
    # 图集类
    class Album:
        def __init__(self, ab_name, ab_link, ab_path, image_name_pre):
            self.ab_name = ab_name
            self.ab_link = ab_link
            self.ab_path = ab_path
            self.image_name_pre = image_name_pre
    ```
    - 在目标网址上找到图集，然后根据界面元素信息创建图集对象列表：
    ```python
    # 在指定的url链接页面上找到指定xpath路径的元素列表，并生成 Album类对象列表
    def get_ab_obj_list(url, xth_list):
        ab_obj_list = []
        option_headless = webdriver.ChromeOptions()
        option_headless.add_argument("headless")
        driver_chrome = webdriver.Chrome(chrome_options=option_headless)
        driver_chrome.get(url)
        time.sleep(5)
        for i in xth_list:
            ab_ele = driver_chrome.find_element_by_xpath(i)
            ele_ab_name = ab_ele.find_element_by_class_name("img-responsive").get_attribute("alt")
            ele_ab_link = ab_ele.get_attribute("href")
            ele_ab_path = "./" + str(ele_ab_name) + "/"
            ele_ab_image_name_pre = str(ele_ab_name) + get_now_time()
            ab_obj_list.append(Album(ele_ab_name, ele_ab_link, ele_ab_path, ele_ab_image_name_pre))
        driver_chrome.quit()
        # 调试输出
        print("图集对象列表已生成，共：{0}个图集对象".format(len(ab_obj_list)))
        return ab_obj_list
    ```
    - 在创建图集列表的时候要先生成图集元素的xpath列表
    ```python
    # 获取指定数量的图集Xpath路径列表
    def get_abs_xpath_list(xth, num):
        '''
        :param xth: "/html/body/div[1]/div[1]/a[{}]"
        :param num: 5 图集数量，后面应该要做成参数吧
        :return: list
        '''
        l = []
        for i in range(1, num+1):
            l.append(xth.format(i))
        # 调试输出
        print("图集的xpath列表已生成，共：{0}个图集".format(len(l)))
        return l
    ```
    - 然后就是对每个图集对象页面中的图片下载，因为保存图片的时候以时间命名，
    所以每下载一张就等待1s这样可以避免文件下载时被覆盖。
    ```python
    # 下载图集对象的图片
    def get_image_save(h, abo, xth2):
        '''
        :param h: 浏览器header
        :param abo: 图集类实例
        :param xth2: 图集中目标图片的xpath（含通配符）
        :return:
        '''
        # ab_obj_list = []
        option_headless = webdriver.ChromeOptions()
        option_headless.add_argument("headless")
        driver_chrome = webdriver.Chrome(chrome_options=option_headless)
        driver_chrome.get(abo.ab_link)
        time.sleep(10)
        # 图片保存路径：
        p = abo.ab_path
        # 图片元素列表：
        image_ele_list = driver_chrome.find_elements_by_xpath(xth2)
        for i in image_ele_list:
            try:
                image_i = i.find_element_by_class_name("img-responsive").get_attribute("src")
                image_d_urllib(image_i, h, p)
            except:
                print("第{}个图片未找到src".format(i.text))
                continue
            finally:
                time.sleep(1)  # 等待1s 使图片文件名尾号+1
    ```
    - 图片下载的方法写在下面一节。
    
---

#### 二、图片下载方案：
尝试下载下面链接的图片：
`target_image:http://bizhi.zhuoku.com/wall/jie/20070409/huoying/014.jpg`
http://nen.mnihao.com/up/nenmb/img/180228/1802280845395a95fbb32d382/5a95fbb6a8a4a.jpg
> 因为如果使用request.get之类的方法，就不像selenium使用webdriver一样模拟浏览器操作，
所以如果要打开某个链接最好加上浏览器header，这样可以防止目标网站把我们的请求给屏蔽了。 
- 找到我们之前在`Python`中使用的浏览器`header`：
    ```
    headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;"
          "q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "max-age=0",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
              "AppleWebKit/537.36 (KHTML, like Gecko) "
              "Chrome/77.0.3865.90 "
              "Safari/537.36",
    }
    ```
- 下面是我访问某图片网站时浏览器的`header`：
    ```
    Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3
    Accept-Encoding: gzip, deflate
    Accept-Language: zh-CN,zh;q=0.9,en;q=0.8
    Connection: keep-alive
    Cookie: UM_distinctid=16d8cf4a28a1e9-0146ea213da09b-67e1b3f-1fa400-16d8cf4a28b73d; CNZZDATA1271287794=1491433848-1570027769-%7C1571413173; Hm_lvt_0a1ff31ae133400f7d3eb6ef294dfe46=1570804479,1570940604,1571371144,1571416974; Hm_lpvt_0a1ff31ae133400f7d3eb6ef294dfe46=1571416980
    Host: www.nenmp.com
    Referer: http://www.nenmp.com/
    Upgrade-Insecure-Requests: 1
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36
    ```
- 浏览器`header`解析：
    - `Accept`:
        >- 作用：浏览器端可以接受的媒体类型。
        >- 例如：`Accept: text/html` 代表浏览器可以接受服务器回发的类型为 `text/html` 也就是我们常说的html文档,
如果服务器无法返回text/html类型的数据,服务器应该返回一个406错误 `non acceptable` ，通配符 `*` 代表任意类型。
例如 `Accept: */*` 代表浏览器可以处理所有类型，（一般浏览器发给服务器都是发这个）

    - `Accept-Encoding`：
        >- 作用：定义客户端可以理解的编码机制，浏览器申明自己接收的编码方法，通常用来指定压缩方法，是否支持压缩，支持什么压缩方法（gzip，deflate），
        >- 例如：`Accept-Encoding: gzip,compress`
    - `Accept-Language`：
        >- 作用：浏览器申明自己接收的语言列表
        >- 语言跟字符集的区别：中文是语言，中文有多种字符集，比如`big5`、`gb2312`、`gbk`等
        >- 例如：`Accept-Language: zh-CN,zh;q=0.9,en;q=0.8`
    - `Connection`：
        >- 作用：表示是否需要持久连接，用于表明当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会不会关闭的通用头标。HTTP 1.1默认进行持久连接。
        >- 例如：`Connection: keep-alive` 当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接保持连接，客户端再次访问这个服务器上的网页，
        会继续使用这一条已经建立的连接
        >- `Connection: close`一个Request完成后，客户端和服务器之间用于传输HTTP数据的TCP连接会关闭，当客户端再次发送Request，需要重新建立TCP连接.
    - `Host`：
        >- 作用: 请求报头域主要用于指定被请求资源的Internet主机和端口号，它通常从HTTP URL中提取出来的
        >- 例如: 我们在浏览器中输入：`http://www.hzau.edu.cn`浏览器发送的请求消息中，就会包含Host请求报头域，
        如下：`Host：www.hzau.edu.cn`此处使用缺省端口号80，若指定了端口号，则变成：Host：指定端口号
    - `Accept-Charset`:
        >- 浏览器可以接受的字符编码集
        >- 例如：`Accept-Charset: gb2312,utf-8;q=0.7,*;q=0.7`
    - `Referer`：
    - ...后面用到的话再来补充吧！~


- 方案一，使用Python标准库中的`urllib`模块：  
    - 通过urllib.request.Request(url, headers=header)建立请求对象
    - 通过urllib.request.urlopen(请求对象)获得响应，写到本地保存为图片文件就OK了
    - 请示要加上header不然，你懂的！~
    - 方法成功了[`image_download.py`](./image_download.py)，但是有一个问题
        > 在用`urllib.request.urlopen`请求桌酷网的图片时只能请求成功一次，后面再发请求，
        网站给我返回的都是下面这个广告图片，不知道为什么？  
        ![](./2019102022425.jpg)
        
        > 我把原图片链接放在这里，应该是[小樱](http://bizhi.zhuoku.com/wall/jie/20070409/huoying/014.jpg)。  
        经过尝试，终于找到问题出在哪里，在我的请求报文头里，也浏览器直接请求不同的地方有三个：
        - 1、我请求时未添加`"If-Modified-Since"`：
            - 因为我看我之前的响应，有304状态码，所以我在想是不是因为我没有上送`If-Modified-Since`导致的？
            但是当我尝试添加`If-Modified-Since`之后也是同样的问题，甚至得到不正确的响应。所以我断定应该不是这里的问题。
        - 2、我请示时未添加`Host`和`Referer`：
            - 我先把这两个字段都添加上去，按照浏览器可以正常访问的情况添加（此时上面那个`If-Modified-Since`还未删除）。
            添加好之后再请求，响应就正常了，也可以正常下载图片。
            - 然后把`If-Modified-Since`删除，再次请求，响应仍然正常。
            - 再把`Host`删除，再次请求，响应仍然正常。
            - 再把`Referer`删除，再次请求，响应就出现了上面描述的情况了，下载了一个“档板图片”，
            当我把`Referer`重新添加上去，就正常了。  
            [# image_download.py](./image_download.py)
            ```
            # 浏览器header
            headers = {
                "Accept": "text/html,application/xhtml+xml,application/xml;"
                          "q=0.9,image/webp,image/apng,*/*;"
                          "q=0.8,application/signed-exchange;"
                          "v=b3",
                "Accept-Encoding": "gzip,deflate",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                # "If-Modified-Since": "Thu, 05 Sep 2019 07:51:15 GMT",
                # "Cookie": "bdshare_firstime=1571414387023; cck_lasttime=1572056394373; cck_count=1",
                # "Cache-Control": "max-age=0",
                "Connection": "keep-alive",
                # "Host": "www.zhuoku.com",
                "Referer": "http://www.zhuoku.com/zhuomianbizhi/star-starcn/20191024155759(3).htm",
                "Upgrade-Insecure-Requests": "1",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) "
                              "Chrome/77.0.3865.90 "
                              "Safari/537.36",
            }
            ```
        > 上面这个问题解决后，就开始考虑如何把 `Referer` 和访问目标页面关联起来。但是目前就发现[“桌酷”](http://www.zhuoku.com)是这样的。
        
        
        
- 方案二，使用requests库
